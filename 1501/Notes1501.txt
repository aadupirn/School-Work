COE 1501 Notes
Check website for office hours and stuff http://people.cs.pitt.edu/~nlf4/cs1501/

1/6/16
Office Hours   T 1-2:30
		W 1:30-3
		H 1-2:30

all emails have [CS1501] in subject

Algorithm metrics: time, memory, complexity.

asymptotic performance: performance as input size goes to infinity

Runtime determined by two factors: 
1 cost of executing each statement
2 frequency of execution of each statement

time	frequency

grey			t0	1

yellow		t1	n

blue			t2	n(n-1)/2

red			t3	n^3/6 - n^2/2 + n/3

purple 		t4	n^3/6 - n^2/2 + n/3 (worst case)

using summations for blue red and purple

1/11/16 

- always try to model for worst case

- worst case for threesum example

	t0 + t1n + t2(n^2/2-n/2) + t3(n^3/6-n^2/2+n/3) + t4(n^3/6-n^2/2+n/3)

- Ignore multiplicative constants and lower terms

- O(n^3) <-- big O notation
	- upper bound on asymptotic performance 
- Big Omega 
	- Lower bound on asymptotic performance
- Theta
	- Upper and Lower bound on asymptotic performance.
	- Exact bound
- ALL THREE ARE ALWAYS IN THE WORST CASE.

- Formal definitions

	f(x) is O(g(x)) if constants c and x0 exist such that:
		|f(x)| <= c*|g(x)| as x>x0.
		- x0 is ensurring sufficiently large input
	f(x) is OMEGA(g(x)) if constants c and x0 exist such that
		|f(x)| >= c*|g(x)| as x > x0 
	if f(x) is O(g(x)) AND OM(g(x)) then f(x) is THETA(g(x))
		c1 and c2 and x0 exust such that both above things are true.
- Tilde approximations and Order of Growth
	- Order of Growth?
		Upper bound?
			O(n^3)
		Lower bound?
			c1 = 1, c2 = 1/48
			g(x) = x^3 works for big oh and omega
		THETA(n^3) works.
- Tilde approximations is a THETA bound with multiplicative constants.

- That was easy but lower bounds are not always that easy to prove.

- Common orders of grouth
	- Constant - 1
	- Logarithmic - log(n)
	- Linear - n
	- Linearithmic - nlog(n)
	- Quadratic - n^2
	- Cubic - n^3
	- Exponential - 2^n
	- Factorial - n!

- Back to ThreeSum, is there a better way?
	- Pick 2 numbers and then binary search for the third. 
	- n^2(log(n))
	- once its sorted then you can do it in n(log(n)) since you only have to sort once.

- Sorting
	GIven a list of n items place the items in a given order.
		Numerical, alphabetical.
	There are good, bad, and ugly ones.
	- less(x, y) returns true if x<y.
	- exch(a[], x, y)
		swaps the values and indexes x and y.
- Bubble sort
	-Compare pairs of items, swap if they are out of order. Repeat until you make 0 swaps.

1/13/16

-Bubble Sort
	-each value bubbles up to where it should be
	-improved bubble sort only goes to n, n-1, n-2 as each last element becomes sorted.
		-still O(n^2). Asymptotically it really doesn't improve runtime.

-Insertion Sort
	-Look at each item in the array and push it as close to the front of the array as it should go.
	-Numbers are being pushed towards the beginning after being compared backwards.

	for (int i = 1; i<n' i+=){
		for (int j=i; j>0 && less(a[j], a[j-1]); j--){
			exch(a, j, j-1)
		}
	}

	- n^2/2-n/2 still O(n^2) better because only in the very worst case.
	-average case is still O(n^2).
	-practically better than bubble sort.

-Merge Sort
	-divide and conquer
	-keep dividing and then once you get to the bottom combine up into the correct order.
	-you're going to have to use multiple arrays.
	-going to divide log2(n) times and then combine arrays up.
	-code is on class website. Going to use 2 arrays so you'll need O(n) extra space for aux array.
	-stable?

-Quick Sort
	-choose a pivot value
	-place the pivot in the array such that all items at lower indices are less than pivot and all higher indices are greater.
	-recurse for lesser indices and greater indices.
	-keep incrementing towards the middle if you find two that should swap swap them the recursively call for left and right.
	-If you keep picking really bad pivots then you'll get O(n^2)
	-Average case will be O(nlog(n))
	-choosing pivot will have drastic impact on runtime.
		-you can scramble indexes.
		-you can pick 5 numbers and choose the median.
	-you won't need extra memory to run quick sort.
	-very quick in practice and widely used.
	-this implementation of quick sort is not stable.

-Stable: stable sorting maintains the relative ordering of tied values

-Comparison sort runtime of O(nlogn) is optimal.
	-The problem of sorting cannot be solved using comparisons with less than n log n time complexity.

1/21/16

-Consider least significant digit.
	-group numers with same digit.	
	-maintain relative order.
	-put back in array. 1's 10's 100's.
	-Called Radix sort.
	-O(nk)
		-where k is the length of the strings.	
	-In-place?
		-no
	-Stable?
		-yes
		-the algorithm uses stability to work.
-Best way to sort 1,000,000 32 bit integers.
	-4 MB
	-no big deal
-What about 1TB of numbers.
	-won't all fit in memory.
	-we have so far had the assumption that we are performing internal sorts.
		-everything in memory.
	-we now need to consider external sorting.
		-writing to disk.
-Hybrid merge sort.
	-read in amount of data that will fit in memory.
	-sort it in place (quick sort)
	-write sorted chunk to disk.
	-repeat till all data is sorted in sorted chunks.
	-merge chunks together.
-external sort considerations.
	-should we merge all chunks together at once?
		-means fewer disk read/writes.
			-each merge pass reads/writes every value.
		-also more disk seeks.
-what about when you have 1PB of data?
	-In 2008, Google sorted 10 trillion 100 byte records on 4000 computers in 6 hours and 2 minutes.
	-48.000 hard drives were involved.
		-at least 1 disk failed during each run of the sort.
-Brute-force Search (exhaustive search)
	-find the solution to a problem by considering all potential solutions and selecting the correct one.
	-run-time is bounded by the number of potential solutions.
	-short passwords are insecure because of vulnerability to brute force.
-Pin cracking example.	
	-will enumerate 10^n different PINs
	-so 10,000 different PINs
	-for a computer this is tiny.
	-what would be long for a computer?
	-say 128 bits
	-2^128
		-assuming a supercomputer can check 338600000000000 passwords per scond stil would take 1.57x10^17 years by brute force.
		-you can trim down impossible combinations from the bits like the ascii for backspace.
	-Pruning!
		-clipping away subtrees of our serch space.
		-when we can use it it makes our algorithm practical for much larger n's
		-does not howerver improve the asymptotic performance of an algorithm.
	-In general exhaustive search trees can be easily traversed with recursion.
-8 queens problem
	-place 8 queens on a chessboard such that no queen can take another.
	-64 C 8
		=64!/(8!*56!) > 4,000,000,000
	-pruning
		-solutions only have one queen per column.
		-solutions can only have one queen per row.
			-now at 8!
		-can only have one queen per diagonal.
	-Basic idea
		-recurse over columns of the board.
		-each recursive call iterates through the rows of the board.
		-check rows/diagonal
		-are we safe?
			-place queen in the current row
		-if not go to next row/collumn.
-Another problem: Boggle.
	-have 8 different options from each cube	
		-from B[i][j] 8 options
	-naively runtime is 16!
	-pruning
		-from edge of the board you have 5 options and 3 from corners.
		-can't go over the same cube twice.
		-can we possibly build a word off of the letters I have?
	-Implementation concerns with boggle.
		-constructing the words over the course of recursion will mean building up and tearing down strings
		-pushing/poping stack operations are generally THETA(1)
		-Java strings, however, are immutable
			-StringBuilder to the rescue!
			-StringBuffer if you are multithreaded.
1/25/16
-symbol tables
	-abstract structures that link key's to values;
	-key is used to search the data structure for a value
	-key functions: 
		put()
		contains()
-searching through a collection
	-unsorted array
		-step through an array.
	-sorted array
		-binary search
	-unsorted and sorted linked list
		-no random access so sorting will not help. step through array.
	-binary search tree
		-n in worst case but log(n) in average case.
	-4 options at each note in a bst
		-note ref is null
		-k is less
		-k is greater
		-k is equal
-Digital Search Tree: Lets go left and right based on the bits of the key.
	-if first bit is 0 store on left if one store on right
	-value is added to each of the node objects.
	-runtime = bit length
	-still need to compare to check if you have found the number.
-Radix search tries (RSTs)
	-trie as in retrieve.
	-store at full bit address with the same strategy as digital search tree.
	-always bitlength.
	-characters?
		-use ascii, sure.
	-strings
		-huge bitlength.
-In our binary based radix search trie we considered one bit at a time.
	-branch based on more bits?
	-you are going to miss sometimes.
	-repeating letters.
	-miss times --> logr(n)
	-r is size of alphabet
	-Average 20 checks for 2^20 keys in an RST
	-with 2^20 keys in an r-way trie assuming 8-bit ASCII? 2.5.
		-bigger the number of options the lower the miss times.
-TrieSt.java
	-r-way trie.
	-basic node object with value atribute 
	-array added on
	-tooooons of wasted space.
-De La Briandais tries (DLBs)
	-replace the .next array with the r-way trie with a linked list
	-no wasted space!
	-search insert are not THETA(kr)
		-implementations with a log of sparse nodes use dlb otherwise r-way is ok.
	-S-->H-->E-->^
			|
			V
			E-->L-->L-->S-->^
				  |
				  V
				  A-->^

1/27/15

-So far we've continually assumed each serch would look for the presence of a whole key, what about prefix search like Boggle.
-What if data needs to be stored on disk
-You're writing software that will be used to store records of online store transactions, each with a unique ID.
	-vinyl album sales.
-You'll want to store these on disk on non-volatile memory.
-how can you still efficiently search for different things?
-Data stored on disk is grouped into blocks.
	-typically of size 4KB
-I/O to the disk is performed on the block lebel.
-to read a file the os will fetch all of the blocks that store some portion of that file and read the data from each block.
-B-trees
	-operates similarly to a binary search tree, but not limited to a branching factor of 2.
	-the order of a B-tree determines the max branching factor.
	-Invariants for the order M B-tree
		-Nodes have a max of M children
		-M keys per node and must be half full (interior).
		-interior nodes have at min of M/2 children
			-Nodes that are not root or leaves.
	-Start with a single node
	-add keys until the node fills
	-visualization http://pearl.ics.hawaii.edu/~sugihara/courses/ics311f10/notes/BTrees.html
	-when inserting into full leaf split the leaf node and put the middle key into the one above
-runtime
	-logm(n)
-Align m so each block is one node.
	-search and insert are same runtime.

1/29/16

-tries use keys with values.
-use binary bits to create binary search tree. (Radix search trie O(bitlength))

2/3/16

-modulous is good for hash tables.
-if a pigeon's hole is taken, it has to find another.
-if h(x) = h(y) ==i
	-if x is at y 
	-if we want to insert y we must try alternative indices.
	-y is not at HT[h(y)]
-linear Probing
	-attempt to insert the key at index i + 1
	-until an open space is found.
	-searching and inserting has worst case O(n)
	-deleteing is a pain.
-search: 
	-if another key is stored at index i
	-check i +1, +2, +3 until
	-key is found
	-empty location is found
	-hash table is full when circled back to index i

2/8/16

abracadabra!
A - 5
R - 2
D - 1
C - 1
B - 2
! - 1

Two trees with minimum weight

-Check notes to see how to build a huffman tree.
-use radix search trie for decoding
-radix search trie uses the key as the address.
-binary search tree has key in node with address
-encode using an array thie size of the alphabet.
-need to efficiently be able to select lowest wight tress to merge when constructing the trie.
	-can accomplish this using a priority queue
-need to be able to read/write bitstrings!
	-unless we pick multiples of 8 bits for our codewords, we will need to read/write fractions of bytes for our codewords.
	-we'll maintain a buffer of bytes and perform bit processing on this buffer.
-to create the trie create a bitstring. code is in slides.

-Huffman pseudocode.
	-read input
	-compute frequencies
	-build trie/codeword table.
	-write out trie as a bitstring to compressed file.
	-write out character count of input
	-use table to write out codeword for each input character.
	-GET THE REST OF THIS FROM SLIDE
-how do we determine character frequencies?
-option 1 preprocess the file to be compressed.
	-upside best algorithm for best output for the given file.
	-downsides.
		-requires two passes over input, one to analyze frequencies and build the lookup table and another to compress to file.
	-trie must be stored with the compressed file reducing the quality of compression.
	-more stuff in slides.
-Option2 : use a static trie;
	-analize multiple sample files, build a single treee that will be used for all compressions/expansions
	-saves on storage overhead...
	-in general not a very good approach.
	-you can create more data.
-Option 3: Adaptive Huffman coding.
	-single pass over the data to construct the codes and compress with no background knowledge of source distribution

2/15/15

-check online for lzw info
-array for expansion
-trie for compression.
-Further implementation issues: codeword size
	-using fewer bits: gives better compression early on.
	-but leaves fewer codewords available which will hamper compression later on.
-use more bits: greater compression later on
-variable width codewords:
	-technique is diferent.
	-start out using 9 bit codewords.
	-when codeword 512 is inserted switch to outputting/grabbing 10 bit codewords.
	-then 11, then 12, then 13.
	-what happens when you run out of codewords?
-two primary option.
	-stop adding new keywords, use the codebook as it stands.
	-if file changes, it will not be compressed as effectively
	-or
	-throw out the codebook and start out from single characters.
-In general, LZW does better than huffman.
	-also better for compression of archived directories of files.
	-why?
		-very long patterns can be built up, leading to better compression.
		-different files don't "hurt" each other as they did in huffman.
-Lossless compression apps use LZW.
-gifs can use it	
	-and pdf.
-modt dedicated compression applications use other algorithms.
	-DEFLATE (combination of LZ77 and Huffman)
		-Used by PKZIP and gzip.
	-burrows-wheelers transforms.
		-used by bzip2
	-LZMA
		-used by 7-zip
	-brotli
		-introduced by Google in Sept. 2015
-How much can they compress a file?
	-how much can a file be compressed by any algorithm?
-No algorithm can compress every bitstream.
	-assume we have such an algorithm.
	-we can use to compress its own output.
	-you should be able to compress from 1 bit to 0 which by definition has losless compression.
-Use concept of shannon entropy.
-Information theory.
	-Entropy is a key measure in information theory.
	-slightly different from thermodynamic entropy.
	-a measure of the unpredictability of information content.
	-by losslessly compresssing data we represent teh same information in less space.
-Translating a language into binary, the entropy is the average number of bits required to store a letter of the language.
	-entropy of a message * length of a message = amount of information contained in the message.
-On average, a clossless compression scheme cannot compress a message to have more than 1 bit of information per bit of compressed message.
-Uncompressed English has between 0.6 and 1.3 bits of entropey per character of the message.
-truely random data is not compressible.
-entropy tells us how good out compression algorithm really is
-uncompressed data has much lower entropy than compressed data.
-entropy: measure of unpredictability of information.
-weissman score.
	-made up for silicon valley.
	-Weissman is a professor at stanford.
	-one of his students was brought in for Silicon Valley.

-missing notes here check his slides.
-KMP

2/22/24

-improve average case of string pattern matching.
-if you get letter not in pattern slide past it
-if it doesnt match up but could in the future slide to the next starting character.
	-create a right array
	-complete array of -1's except for the locations for each character in the pattern.
	-that characters spot in the right array is set to the index of its right most occurrence.
	-if index of character checked is negative one you can move past the entire check.
	-check right most first.
	-if the letter is in the pattern move so it matches up with the index in the right array.
-Hashing was cool, let's try using that.
	-practically worse than brute force.
	-look at horners method in slides.
	-rolling hash moves down the letters.
	-can get next hash in constant time.
	-Rabin-Karp is what the rolling hash is called.
	-what about collisions?
	-you can set a huge q since you never need to store a hash table.
	-chance of collisions are verrrrrrry small but not 0.
	-so character by comparison on a collision just to be sure.
		-worst cast?
		-THETA(mn) but suuuuuuuuper unlikely.
		-probably fast
		-Las Vegas
	-you could never check
		-guaranteed fast
		-probably fast
		-Monte Carlo

3/21/16

-Prim's algorithm
	-O(V^2)
	-2THETA(V^2) = THETA(V^2)
-use priority que instead of best edge array.
-If you have an array that you are using to find the min of something use priority que!
-when you visit a new node add all of its edges to a pq.
-eager prim's is indexable pq.
-lazy prim's is when you don't use an indexable priority que. THETA(ElgE)
-eager prim's THETA(ElgV)
	-updates take things in the order of V but you still have to look at every edge.
-weighted shortest path.
	-Dijkstra's algorithm.
	-set a distance value of MAX_INT for all nodes but start.
	-set cur = start.
	-whilce destination not visited:
		-for each univisited neighbor of cur
			-compute tentative distance from the start to unvisitited through current.
	-can't have negative edge weights.
	-for minimum tenative distance array use an array. 
	-ElgV

3/23/16

-Back to Minimum Spanning Tree's, Another one
-Kruskals
	-insert all edges to PQ
	-insert until they create a cycle.
	-elge unless cycle detection takes more than lge
-Union Find
-Dynamic connectivity problem
	-For a given graph G can we determine wheter or not two vertices are connected in G?
	-can also be viewed as checking subset membership.
		-connected nodes are viewed as subsets.
-count keeps track of number of nodes in graph.
-connected(int p, int q);
	-return find(p) == find(q);
-Simple approach
	-have an id array simply store the component id for each item in the union data structure.
	-at start each vertex is in its own croup.
	-for every edge union the two vertices and make their id the same.
-What if we store our connected components as a forest of trees?
	-whenever you connect two things you connect thir trees
	-just check the root of the tree to see what connected component you are a member of.
	-lg(v) runtime for find.
	-value in id array can just hold the parent node.
	-if the value stored at index is the same as the index you are at a root.
	-every vertex only has one parent.
-Runtime?
	-weighted trees will make more balanced trees
-to improve even more you can shorten trees by making everything in the tree point directly to the root.
	-in Kruskal's you can just check every time you add an edge and see if it is already connected by using the union find scheme.
	-as long as you have fewer vertices than edges kruskals is elge.

-3/30/16
-Network Flow
	-maintain a que
	-visit top of que after each node is visited
	-once you find an augmenting path
		-find the edge in the path with the lowest residual capacity.
		-add flow to the path as much as you can 
		-start traversal over
	-can't see edges that are saturated
	-once you've gotten through the path an dyou don't see t anymore you are done.
-st cut on g is a set of edges in g that, if removed, will partition the vertices of g in to two sets once with s and one with t.
-focus on finding minimum st cut.
	-Max Flow == Min Cut
-Greatest common divisor.
	-largest in that evenly divides both a and b.
-Easiest approach
	-Brute Force
		-THETA(min(a,b));
	-liinear in value of min(a,b);
	-EXPONENTIAL IN N IF N IS NUMBER OF BITS.
-Euclids algorithm
	-GCD(a, b) = GCD(b, a%b)
	-its linear in bitlength.

-4/4/16
	-Integer multiplication
	-hardware mul instruction has 64 bit inputs and so you have to implement your own.
	-go through each collumn and then figure out the output for each collumn one at a time. (helps with size)
	-for Speed
		-try to divide and conquer
		-break our n-bit integers in half
	-We'll use a recurrence relation to analyze the recursive runtime.
	-keep splitting it up until you can fit multiplications into stuff that fits into the hardware.
	-the master theorem.
		-LOOK AT SLIDES FOR MASTER THEOREM
	-Master theorem will be provided we need to know how to use it.
	-T(n) = aT(n/b) + f(n)
	-by reducing number of recursive calls you can reduce runtime. 
	-at 3000 bits the karatsuba algorithm will become practically better.

-Exponentiation
	-x^y
		-multiply x y times
	-Runtime
	-single loop from 1 to y
	- n is bitlength of y
	-increasing n by 1 doubles the number of iterations
	-THETA(2^n)
	-THIS IS RIDICULOUSLY BAD.
	-How do we get better?
	-divide and conquer
		-(x^(y/2)^2)
	-bounded by multiplying huge numbers

-4/6/16
-Cryptography-enabling secure communication in the presence of third parties
	-Alice wants to send bob a message without anyone else being able to read it.
	-people who aren't bob and alice who might be able to see the message are adversaries.
	-adversary will have access to 
		-ciphertext
		-encryption algorithm
		-Assume enemy knows the system.
-Early classic encryption scheme
	-Caesar cipher.
	-"shift" the alphabet by a set amount.
	-"key" is the amount shifted.
	-Easy to crack with brute force.
-Instead maybe permute the alphabet.
	-still easy
	-look for patterns
	-essentially playing wheel of fortune.
-What is a good cipher?
	-one-time pad
		-list of one-time use keys called a pad
	-to send a message
		-take an unused pad
		-use modular addition to combine key with message
			-for binary xor
		-send to recipiend
	-upon receiving a message
		-take next pad
		-reverse process
		-read result
	-perfect secrecy.
	-Once you run out of pads, no more messages can be sent.
-Symmentric ciphers
	-users share a single key.
	-key can be reused.
-Encryptions/decryptions will be fast
	-typically linear in size of the input
-Ciphertext should appear random
	-best way to recover is a brute force attack on the encryption key.
	-Alice and bob have to both know the same key.
	-key can't be the same key that alice uses with others.
-RSA cryptosystem.
	-everyone will have 2 keys
	-public and private key.
		-public key is two numbers which we will call n and e
		-private key is a single number we will call d
	-Alice wants to send a message to bob
		-Looks up Bob's public key
		-convert the meesage into an integer: m
		-Compute the ciphertext c as
			-c = m^e(modn)
		-send c to bob
	-Bob decrypt
		-m = c^d(modn)
		-m is original message
	-n e and d need to be carefully generated.
	-To generate
		-choose 2 prime numbers p and q
			-n = p*q
			-......... FML
			-knowing proof is not required for the course.
			-knowing how to generate RSA keys and encrypt/decrypt IS.
		-NEXT Why RSA is Secure?
			
4/13/16

-P and NP problems
	-P - problems that can be solved by a deterministic Turing machine using a polynomial amount of computation time, or polynomial time.
	-NP - solvable in polynomial time by a nondeterministic Turing machine. A P-problem (whose solution time is bounded by a polynomial) is always also NP.
-5 options for how these problems intersect.
	-either P is a subset of NP OR P = NP.
		-One of the biggest unsolved problems in computer science.
-intractable = problem is going to require exponential work.
-If P = NP
	-most widely-used cryptography would break.
		-efficient solutions would exist for:
			-breaking public key cryptography	
			-breaking AES/destination
			-Reversing cryptographic hash functions
	-Efficiency would become a lot better
-If P!= NP
	-meh
		-most on going research assumes this to be the case. 
-NP-Complete
	-"hardest" problems in NP
	-equally hard
	-So if we find a polynomial time solution to one of them, we clearly have a polynomial time solution for all of them.
	-How do you show a problem is NP-Complete.
-Show the problem is in NP
	-Show that your problem is at least as hard as every other problem in NP
	-typically done by reducing an existing NP-Complete problem to your problem.
		-In polynomial time.
	-Assume we've just come up with the set cover problem.
		- set of S elements and a collection of s1 .... sm of subsets of S. is there a collection of at most k of these sets whose union equals S?
		-Is it in NP?
			-yep
		-NP-Complete?
			-Take existing NPC problem
			-Vertex Cover problem.
				-Does a vertex cover exist for a graph G that consists of at most k vertices?
		-Always solve the known NP-Complete problem using your new problem.
-A timeline of P/NP
	-1971 - Cook presents the Cook-Levin theorem, which shows that the boolean satisfiability problem is as hard as every problem in NP.
		-It is NP-Complete, bu this term appears nowhere in paper
	-1972-Karp presents 21 NP-Complete problems via reduction from boolean satisfiability.
	-NP-Hard the set of problems at least as hard as NP-Complete. 
-What if you still need to solve an NP complete problem?

4/18/16

	-in general choose mst rather than nearest neighbor since you have guarantee of some sort of max performance.
-Greedy algorithms and Dynamic Programming
-Making change problem.
	-start with quarter then dime then nickle then penny.
	-greedy algorithm
		-each step the algorithm makes the choice that seems best at the moment.
		-nearest neighbor huristic is one.
		-huffman tree
-Nearest neighbor doesn't solve the travelling salesman problem.
	-does not produce an optmial result
-Does our change making algorithm solve the change making problem?
	-yes
	-the future is not impacted by choosing a quarter.
-if coin denominations were 1, 3, 4 and you were making 6 cents this would not be optimal.
-for greedy algorithms to produce optimal results problems must have two properties:
	-optimal substructure: Optimal solution to a subproblem leads to optimal solution
	-The greedy choice property: globally optimal solutions can be assembled from locally optimal choices.
-Finding all subproblems solutions can be inefficient.
	-for fibinacci sequence don't call identical subproblems multiple times.
-memoization.
	-Have array of size n+1;
	-ass array fills up you can just call on the values in the array instead of computing.
	-wait you don't need an array.
	-just the previous and current values.
-Where can we apply dynamic programming
	-optimal substructier
	-overlapping subproblems
		-naively we would need to recompute the same subproblem multiple times.
-Subset sum: given a set of nonegative integers S and a value k is there a subset of S that sums to exactly k?
	-look at notes for recursive code for this problem.
	-dynamic programming is in slides also.
-0/1 knapsack problem
	-knapsack that can hold a certain amount of weight and you want to maximize the amount of weight to carry.
	-each item has 2 choices
		-goes in knapsack or it doesnt.
-Database query optimization
	-SQL is declarative.
		-user's don't specify what the database should do to answer their query, just what they want.
	-Find best blans to join triplets of tables.
-4/20/16 REVIEW!!!

-GCD
	GCD(56, 24)
	24(2) + 8
	GCD(24, 8)
	8(3) + 0
-Union Find
	-Dynamic connectivity or subset problem.
	-fastfind(simple) you just update indeces to be the same.
	-root of the smaller tree should be made a child of the root of the larger tree
-RSA
	Find 2 large prime numbers. (p, q).
	n = p*q.
	PHI(n) = PHI(p) * PHI(q) = (p-1)*(q-1).
	pick e such that e is between 1 and PHI(n) and e and PHI(n) are coprime.
	determine d such that d = e^(-1)mod(PHI(n))
	
	-public key = (n, e)
	-private = (d)
	
	given a message:
	= c = (m^e)mod(n)
	
	message = c^(d)mod n
	
-Random large prime number
	-randomly choose large prime number
	-check if it is not prime or could be prime.
	
-Recurrence relation
	T(n) = aT(n/b) + f(n)
	b is how much we are dividing our input size by in further operations.
	aT(x) + f(n) 
	x is the size of the input that you pass on.