
-Challenges
    communication
    synchronize
    scale
    how do you deal with failure

-Indexer
    reads through the web data generated by the crawler and (among other things) creates a reverse index from keywords to pages.

-MapReduce
    way to program a distributed system.
    the input is a list of (k1, v1) key-value pairs.
    The programmer specifies:

        a map function map(k1, v1) -> list(k2, v2)
        performs some computation on a single key value pairs and generates new key value pairs.

        a reduce function reduce (k2, list(v2)) -> list(v3)

    Most important thing that the scheduler can do is try to run a worker on the machine that has the data on it.

-Protocol Buffers
    a declarative description of data schema is automatically turned into source code. You compile the data layout of your message with protoc compiler.

-Dealing with data loss: Need to have replication.

-Reminders:
    MapReduce for indexing web pages.
    Protocol buffers for efficient message communication.
    GFS - distributed replicated file storage.
        Many chunks 64 MB
        Single master server for managing metadata.
        on master falure choose new server to take over from saved state.

-Chubby: A course grain lock service.
    One server acquires lock.

-Fault tolerance
    -servers can crash, reboot, be arbitrarily slow.
    -network messages can be dropped, duplicated, delivered out of order.
    -we do NOT consider Byzantine faults.

-Byzantine Faults:
    Incorrect or inconsistent messages are sent, possibly maliciously.

-Paxos Protocol
    algorithm for arriving at consensus for a single value (e.g., which node should be master, what is the value of a log entry)
    does not require a "master" node (although such a node makes the protocol more efficient)
    works as long as the majority of nodes are fault-free (an no Byzantine faults)
    requires that nodes be uniquely numbered
    can take arbitrarily long time to reach consensus given pathalogical pattern of node failures.
    nodes take on two distinct roles: Proposer and Accepter.
        the protocol is more efficient if only one node acts as Proposer.

-Paxos
    1. Prepare. Proposer has value to reach consensus on.  A prepare message is sent to the Acceptors with S that is the number of the Proposer.
    2. Promise. The acceptor receives the prepare message and returns a Promise to ignore any future messages with a sequence number less than S.
    3. Once proposer receives a promise message from a majority of acceptors it can set the value of the proposers.
    4. Accept. If the Acceptor receives an Accept Request with sequence number S, it must accept it as long as it has not sent a Promise message with a larger S
    The message is accepted by registering the associated value and responding with an Accepted message to the Proposer.
    5. Accepted. If the Proposer receives Accepted messages from a majority of Acceptors, we have reached consensus. If not, the algorithm can start over with a higher sequence number.

-Almost everyone uses Paxos in some form.

-Why does this matter.
    Interesting
    Appreciation of the difficulties involved in designing the scalable fault-tolerant infrastructure you use for scalable machine learning for big data biology.
    Try to avoid synchronization (locks). It hurts performance.

-Hadoop
    Open source framework for scalable, fault-tolerant distributed computing developed by the Apache Foundation. It is named after a dev's child's toy elephant.
-Hadoop Distriputed File system (HDFS)
    Tolerate hardware failure. (Replication)
    Optimize for streaming data access - emphasize throughput over latency.
    Optimize for large data sets.
    Simple coherency model (write once, read many)
    "Moving computation is cheaper than moving data"
    Portability.

    HDFS Namenode (like google master)
        Single master node for meta-data
        client contacts NameNode to open, close, rename
        NameNode maintains locations of blocks (chunks) of files
        NameNode manages replicas, duplicating data as necessary
        HDFS is "rack-aware" - it knows what racks each server is in (because you tell it in a config file)
        Failure of NameNode crashes HDFS
            -High Availability fail-over setup possible. (maintains "shadow" NameNodes)

    HDFS Datanodes
        Typically every node in cluster runs a datanode server
        client contacts Datanode to read/write blocks
        periodically sends HeartBeat to NameNode to indicate it is still working.
        periodically sends BlockReport with all blocks it is storing to NameNode.

    HDFS Client: File Creation.
        client can set block size and replicaiton factor on file creation.
            default is 128MB and 3 replicas.
            2 replicas are put on different nodes in same rack, third on different rack.
        can also specify storage types (disk vs ssd)
        file is not actually created on Datanode until full block is written (or file closed)

    File Read
        client gets location of block from NameNode.
            reads are serviced from "nearest" node.
            client can find out location of all replicas of a block
            multiple clients can read the same file.
            if nearest node is same node as client can skip going over network.

    File Write.
        files are write-once: they can not be modified after they are written except to append.
            only a single writer per file is allowed
            replica writes are pipelined.
            NameNode gives client a list of datanodes to host block replicas.
                client writes to first datanode.
                first datanode writes to second, while it is possible still receiving data from the client.

    Interface
        supports a commandline interface with many typical commands.
        hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir
        the hdfs://nn1.example.com can be omitted if configured.

        there are Java, C, and REST API's



-Hadoop YARN
    -Yet another resource negotiator.

    1. A client program submits the appliication, including the necessary specifications to launch the application-specific ApplicationMaster itself.

    2. The ResourceManager assumes the responsibility to negotiate a specified container in which to start the ApplicationMaster and the launches the ApplicationMaster

    3. The application master on boot up registers with the ResourceManager-the registration allow the client program to query the ResourceManager for details, which allow it to directly communicate with its own ApplicatioonMaster.

    4. During normal operation the ApplicationMaster negotiates appropriate resource containers via the resource-request protocol.

    5. On successful container allocations, the ApplicationMaster launches the container by providing the container launch specification to the NodeManager. The launch specification, typically, includes the necessary information to allow the container to communicate with the ApplicationMaster itself.

    6. The application code executing within the container then provides necessary information (progress, status etc.) to its ApplicationMaster via an application-specific protocol.

    7. During the application execution, the client that submitted the program communicates directly with the ApplicationMaster to get status, progress updates etc. via an application-specific protocol.

    8. Once the application is complete, and all necessary work has been finished, the ApplicationMaster deregisters with the ResourceManager and shuts down, allowing its own container to be repurposed.
